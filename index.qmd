---
title: "Applying Reinforcement Learning to the Traveling Salesman Problem"
author:
  - name: Victor De Lima
    affiliations:
      - name: Georgetown University
date: '2023-04-27'
format:
  html:
    toc: true
    code-copy: true
    number-sections: true
    code-line-numbers: true
    highlight-style: github
bibliography: assets/references.bib
csl: assets/council-of-science-editors-brackets.csl
---

- Visit my [website](https://www.victordelima.com/).


::: {.callout-note appearance="simple"}

All code used in this report is publicly available on [GitHub](https://github.com/vdelimad/tsb-with-reinforcement-learning).

::: 


### Abstract {.unnumbered}

The Traveling Salesman Problem (TSP) is a well-studied optimization problem with a wide variety of applications and approaches to solving it. The methods and tools of Reinforcement Learning (RL) offer a distinctive strategy for approaching the TSP due to the ease with which its reward structure can be modified. Using data obtained from Skyscanner, this study examines how Q-learning, an RL technique, can optimize a travel route through 42 Asian cities. We first provide a theoretical background into the tools discussed, followed by an exposition of the methodology and experiments. The study's results show that Q-learning is a very effective method for solving the TSP. We also discuss the model's limitations and how it can be extended in future work.


```{python}
#| echo: false

# libraries
from IPython.display import display, HTML, IFrame
```


![Picture of a robot travel agent generated with Stable Diffusion [@stability_ai_stable_2023] using prompt "_a friendly-looking humanoid robot travel agent greeting you on its work desk and a map hanging in the back wall_"](assets/resources/Images/TravelAgents/travel_agent_1.jpeg){#fig-travel-agent fig-align="center" width=75%}


## Introduction

Anyone who has planned a trip that involves visiting several cities will have undoubtedly found themselves wondering what is the best way to visit each location while minimizing the cost of the trip. Each location has several considerations, such as distance, cost, and the importance of visiting specific places. The question can become overwhelming with the realization that we would have to consider each city as both a potential starting point and a potential next stop from any other city. We can analyze such a scenario through the framework of the Traveling Salesman Problem (TSP). Karl Menger, one of the first to study the problem mathematically, defines the TSP as "the task to find, for finitely many points whose pairwise distances are known, the shortest route connecting the points" [@schrijver_history_2005]. In other words, the goal of the TSP is to find the shortest possible route that visits every point once and then returns to the origin. The TSP is an NP-hard combinatorial optimization problem. It is simple to solve by brute force for five stops by trying all combinations (5! problem) but already impossible for 50 (50! problem).
 
The TSP is a well-known and widely studied problem in mathematics and computer science. Beyond travel, the TSP has applications in many fields, including manufacturing, transportation, and engineering. For example, some applications that successfully employ the TSP include overhauling gas turbine engines [@plante_product_1987], X-Ray crystallography [@bland_large_1989], and computer wiring [@lenstra_simple_1975]. Although researchers have studied the TSB extensively and developed several discrete optimization techniques to solve it, Reinforcement Learning (RL) offers many advantages compared to classical optimization methods. Most importantly, RL offers a framework in which it is relatively simple to change the reward structure to adjust the optimization problem under changing circumstances.

In this paper, we approach the TSP from an RL perspective and utilize Q-Learning to optimize a 42-city travel route using real prices obtained from SkyScanner. First, we discuss the theoretical background that motivates the experiment. Then, we explore the methodology and data on which we built the analysis. Next, we review the results, which show that Q-learning is a very effective method for solving the TSP. Lastly, we offer a discussion on the model's limitations and the ways to extend the model in future research.


## Related Work

The TSP is an extension of the Hamiltonian Circuit Problem, a formulation that asks whether a graph contains a cycle that visits each node exactly once, which is itself an NP-complete problem [@miller_reducibility_1972]. The TSP builds on this theory and asks what is the shortest possible Hamiltonian Circuit given a weighted graph. Furthermore, we can broadly divide the TSP into symmetric and asymmetric subcategories in the single-agent case. Given two nodes, A and B, if the costs associated with traveling from A to B are the same as those of traveling from B to A, then the TSP is symmetric. However, if the cost of traveling from A to B differs from that of traveling from B to A, then the TSP is asymmetric [@davendra_traveling_2010].

The body of literature discussing methods for solving the TSP is extensive. The base method is by brute force (also known as the Naive Approach), which consists of finding all possible route permutations. This approach becomes unfeasible as the number of locations increases. The Branch and Bound method attempts to find a solution by breaking the problem into subproblems. Each subproblem is recursively solved to find an optimal solution [@little_algorithm_1963]. The Nearest Neighbor Method is also a widely used heuristic method, which relies on going to the closest location at every step and returning to the starting one once all locations have been visited [@johnson_traveling_1997].


Additionally, there are extensively studied approximate approaches, such as Simulated Annealing (SA), which is inspired by the materials science process of annealing. @kirkpatrick_optimization_1983 initially conceived SA and consists of randomly generating an initial solution and iteratively improving it by selecting a neighboring alternative. At each step, the procedure decides whether to accept or reject the selection based on a specified criterion. Lastly, genetic algorithms (GAs) are also very popular. Starting from @brady_optimization_1985, GAs build on the idea of natural selection to start with a set of candidate solutions that are evaluated for "fitness." Then, the process iterates until meeting a stopping criterion.

Lastly, the TSP has also received attention from the rapid increase in reinforcement learning applications in recent decades witnessed in recent decades. Some examples worth mentioning include applying Q-learning and SARSA to optimize vehicle routes with fuel constraints @ottoni_reinforcement_2022, and TSP involving truck-drone fleets coordination using deep reinforcement learning @bogyrbayeva_deep_2023. This study builds on this body of literature to apply Q-Leaning to a TSP flight route optimization problem in the asymmetric case.


## Methodology

### Problem Formulation

In this paper, we employ the asymmetric case of the TSP since the costs associated with traveling from airport A to airport B are not the same as the costs of traveling from B to A, given differing fare rates. Furthermore, itineraries might include varying stopovers that lead to travel distances between the points that are not symmetrical in the model. We formulate the TSP as a set of locations $V=\left\{v_1, \ldots \ldots, v_n\right\}$, a set of edges $A=\{(r, s): r, s \in V\}$, and a set of costs associated with each edge $d_{r s}=d_{s r}$, where each cost is associated with an edge $(r, s) \in A$. Since this is the asymmetric case, we allow for $d_{r s} \neq d_{s r}$ for any edge $(r, s)$[@davendra_traveling_2010].

In the Q-learning framework, we aim to to learn which action to take in each location (the state). To this end, the model learns the value associated with each transition from one location to another (referred to as Q-values) by sampling the environment and repeatedly applying theÂ update rule for the Q-learning algorithm:

$$ 
\begin{align}\begin{aligned}Q(s, a) = Q(s, a) + \alpha * \\\begin{split} \left( r +\gamma \max_{a'} Q(s', a') - Q(s, a) \right) 
\end{split}\end{aligned}\end{align} 
$$ {#eq-q-update}


where $Q(s, a)$ is the current estimate of the value of taking action $a$ in state $s$, $r$ is the reward (cost) from taking an action, $\alpha$ is a learning rate hyperparameter, and $\gamma$ is a discount rate hyperparameter. States and actions denoted with the $\prime$ symbol denote the next state or action. By applying @eq-q-update, the model can update the existing estimates of $Q(s, a)$. Then, the agent takes the action with the highest Q-value at every step [@bilgin_mastering_2020].

### Data

We obtained the initial list of target cities and coordinate data from @flagpicturescom_list_2023. Pyongyang, Sanaa, Rangoon, Damascus, Kabul, and Thimphu were removed from the list due to limited flight availability, resulting in 42 cities for the study. @fig-airport-locations shows the airport locations in each city.


```{python}
#| echo: false
#| label: fig-airport-locations
#| fig-cap: "Interactive Folium world map with overlay over Asia showing the cities of interest, with a tile by @national_geographic_esrinatgeoworldmap_2023 and administrative boundaries from @opendatasoft_world_2022. **Note**: VPNs may prevent correct map rendering."

width_percentage = "100%"
IFrame(src='assets/resources/Outputs/Plots/map_of_airport_locations.html', width=width_percentage, height=500)
```


We collected these cities' flight prices, duration, and stopover data using the SkyScanner API [@skyscanner_api_2023]. We made several simplifying assumptions in the data collection. We only obtained data for the single, arbitrary date of 22 May 2023. Also, the data does not  include the layover time between segments, only the in-flight time^[For clarification, we cannot take the difference between the first segment departure and last segment arrival to calculate the total itinerary time because flight data is provided in local time, which would not account for time zone changes.]. Lastly, the data collected only corresponds to economy cabin class. Relaxation of these assumptions can be explored in future work.

Given the 42 airports, we completed three transition matrices with 1,722 non-zero entries, each matrix corresponding to either flight price, duration, or stopover information. The entries correspond to each of the $42 \times 42$ transitions minus the diagonal, which would correspond to traveling from an airport to itself ($42 * 42 - 42 = 1,722$). The search resulted in 38,236 segments, narrowed to 15,396 itineraries, and narrowed to the 1,722 'best' flights. To choose the âbestâ flight, we used the heuristic of valuing each flight hour at $20 and every stopover at $50, resulting in:

$$ 
\begin{flalign}\begin{aligned} \rm best \, price =  fare + flight \,time  \\\begin{split} \rm * 20 + stops * 50 
\end{split}\end{aligned}\end{flalign} 
$$ {#eq-best-flight}


@fig-airport-links shows color-coded prices between airports.


```{python}
#| echo: false
#| label: fig-airport-links
#| fig-cap: "Interactive Folium world map showing the links between airports with color-coded prices. **Note**: VPNs may prevent correct map rendering."

width_percentage = "100%"
IFrame(src='assets/resources/Outputs/Plots/map_of_airport_links.html', width=width_percentage, height=500)
```


## Experiments and Results

We implemented the model in Python using custom classes and functions. The model was based on the @alves_da_costa_delivery_2019 implementation, particularly on the method for tracking previously visited locations and preventing revisiting them. The model constructed a matrix of  Q-values of size $42 \times 42$ and employed an e-greedy strategy with epsilon decay at a rate of 0.999. Finally, we ran the model through 200 episodes of Q-learning, with results available in @fig-experiment-results. 


::: {#fig-experiment-results}
![Flight Score Results](assets/resources/Outputs/Plots/flight_score.png){#fig-score-results fig-align="center" width=85%}

![Fare Expense Results](assets/resources/Outputs/Plots/fares_in_usd.png){#fig-expense-results fig-align="center" width=85%}

![Flight Time Results](assets/resources/Outputs/Plots/flight_time.png){#fig-time-results fig-align="center" width=85%}

![Flight Stops Results](assets/resources/Outputs/Plots/flight_stops.png){#fig-stops-results fig-align="center" width=85%}

Experiment Results
:::


The results obtained had several important improvements from the initial randomness, such as:

- Decreasing overall flight expenditure from around $18,000 to $8000
- Decreasing overall flight expenditure from around 20,000 minutes (333.3 hours or 7.93 average hours per flight) to 9000 (133.3 hours or 3.17 average hours per flight)
- Decreasing overall stopovers from around 100 to 60.


Furthermore, @fig-first-and-last-episodes shows the chosen trajectories for the first and last episodes, allowing us to observe the choice changes from the untrained and trained models. It is challenging to discern the disparities between these figures through mere observation, underscoring the model's proficiency in revealing mathematical advantages that may otherwise be arduous to infer by visual inspection alone.

::: {#fig-first-and-last-episodes layout-ncols=1}
```{python}
#| echo: false
#| label: fig-first-episode-links
#| fig-cap: "Shows the first episode's links. **Note**: VPNs may prevent correct map rendering."

width_percentage = "100%"
IFrame(src='assets/resources/Outputs/Plots/first_episode_links.html', width=width_percentage, height=500)
```

```{python}
#| echo: false
#| label: fig-last-episode-links
#| fig-cap: "Shows the last episode's links. **Note**: VPNs may prevent correct map rendering."

width_percentage = "100%"
IFrame(src='assets/resources/Outputs/Plots/last_episode_links.html', width=width_percentage, height=500)
```

Trayectories of the first and last episodes.
:::


## Conclusions

This study began by explaining the TSP and motivated the use of RL as a method for solving it. Then we reviewed the TSP literature and the wide variety of existing solutions, including heuristic, approximate, and RL methods. We then formalized the problem and explained how the Q-learning process works. Next, we covered how the data for the experiment was collected, the assumptions made, and visualized the search space. Lastly, we discussed the results from the experiment showing that Q-learning is a very effective method for solving the TSP, achieving significant gains in cost reduction and time minimization for the desired route through the Asian cities.

Future work may include the relaxation of several assumptions made in this implementation. For example, future implementations may include additional data to capture varying prices across the expected duration of the trip rather than utilizing prices from a single date. Also, more complicated choices of âbestâ flight may be considered to explore their impact on the model.


## References {.unnumbered}
